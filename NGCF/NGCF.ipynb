{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NGCF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8S2rFvXJPdfo"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Created on Oct 10, 2018\n",
        "Tensorflow Implementation of Neural Graph Collaborative Filtering (NGCF) model in:\n",
        "Wang Xiang et al. Neural Graph Collaborative Filtering. In SIGIR 2019.\n",
        "\n",
        "@author: Xiang Wang (xiangwang@u.nus.edu)\n",
        "'''\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import sys\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
        "\n",
        "from utility.helper import *\n",
        "from utility.batch_test import *\n",
        "\n",
        "class NGCF(object):\n",
        "    def __init__(self, data_config, pretrain_data):\n",
        "        # argument settings\n",
        "        self.model_type = 'ngcf'\n",
        "        self.adj_type = args.adj_type\n",
        "        self.alg_type = args.alg_type\n",
        "\n",
        "        self.pretrain_data = pretrain_data\n",
        "\n",
        "        self.n_users = data_config['n_users']\n",
        "        self.n_items = data_config['n_items']\n",
        "\n",
        "        self.n_fold = 100\n",
        "\n",
        "        self.norm_adj = data_config['norm_adj']\n",
        "        self.n_nonzero_elems = self.norm_adj.count_nonzero()\n",
        "\n",
        "        self.lr = args.lr\n",
        "\n",
        "        self.emb_dim = args.embed_size\n",
        "        self.batch_size = args.batch_size\n",
        "\n",
        "        self.weight_size = eval(args.layer_size)\n",
        "        self.n_layers = len(self.weight_size)\n",
        "\n",
        "        self.model_type += '_%s_%s_l%d' % (self.adj_type, self.alg_type, self.n_layers)\n",
        "\n",
        "        self.regs = eval(args.regs)\n",
        "        self.decay = self.regs[0]\n",
        "\n",
        "        self.verbose = args.verbose\n",
        "\n",
        "        '''\n",
        "        *********************************************************\n",
        "        Create Placeholder for Input Data & Dropout.\n",
        "        '''\n",
        "        # placeholder definition\n",
        "        self.users = tf.placeholder(tf.int32, shape=(None,))\n",
        "        self.pos_items = tf.placeholder(tf.int32, shape=(None,))\n",
        "        self.neg_items = tf.placeholder(tf.int32, shape=(None,))\n",
        "\n",
        "        # dropout: node dropout (adopted on the ego-networks);\n",
        "        #          ... since the usage of node dropout have higher computational cost,\n",
        "        #          ... please use the 'node_dropout_flag' to indicate whether use such technique.\n",
        "        #          message dropout (adopted on the convolution operations).\n",
        "        self.node_dropout_flag = args.node_dropout_flag\n",
        "        self.node_dropout = tf.placeholder(tf.float32, shape=[None])\n",
        "        self.mess_dropout = tf.placeholder(tf.float32, shape=[None])\n",
        "\n",
        "        \"\"\"\n",
        "        *********************************************************\n",
        "        Create Model Parameters (i.e., Initialize Weights).\n",
        "        \"\"\"\n",
        "        # initialization of model parameters\n",
        "        self.weights = self._init_weights()\n",
        "\n",
        "        \"\"\"\n",
        "        *********************************************************\n",
        "        Compute Graph-based Representations of all users & items via Message-Passing Mechanism of Graph Neural Networks.\n",
        "        Different Convolutional Layers:\n",
        "            1. ngcf: defined in 'Neural Graph Collaborative Filtering', SIGIR2019;\n",
        "            2. gcn:  defined in 'Semi-Supervised Classification with Graph Convolutional Networks', ICLR2018;\n",
        "            3. gcmc: defined in 'Graph Convolutional Matrix Completion', KDD2018;\n",
        "        \"\"\"\n",
        "        if self.alg_type in ['ngcf']:\n",
        "            self.ua_embeddings, self.ia_embeddings = self._create_ngcf_embed()\n",
        "\n",
        "        elif self.alg_type in ['gcn']:\n",
        "            self.ua_embeddings, self.ia_embeddings = self._create_gcn_embed()\n",
        "\n",
        "        elif self.alg_type in ['gcmc']:\n",
        "            self.ua_embeddings, self.ia_embeddings = self._create_gcmc_embed()\n",
        "\n",
        "        \"\"\"\n",
        "        *********************************************************\n",
        "        Establish the final representations for user-item pairs in batch.\n",
        "        \"\"\"\n",
        "        self.u_g_embeddings = tf.nn.embedding_lookup(self.ua_embeddings, self.users)\n",
        "        self.pos_i_g_embeddings = tf.nn.embedding_lookup(self.ia_embeddings, self.pos_items)\n",
        "        self.neg_i_g_embeddings = tf.nn.embedding_lookup(self.ia_embeddings, self.neg_items)\n",
        "\n",
        "        \"\"\"\n",
        "        *********************************************************\n",
        "        Inference for the testing phase.\n",
        "        \"\"\"\n",
        "        self.batch_ratings = tf.matmul(self.u_g_embeddings, self.pos_i_g_embeddings, transpose_a=False, transpose_b=True)\n",
        "\n",
        "        \"\"\"\n",
        "        *********************************************************\n",
        "        Generate Predictions & Optimize via BPR loss.\n",
        "        \"\"\"\n",
        "        self.mf_loss, self.emb_loss, self.reg_loss = self.create_bpr_loss(self.u_g_embeddings,\n",
        "                                                                          self.pos_i_g_embeddings,\n",
        "                                                                          self.neg_i_g_embeddings)\n",
        "        self.loss = self.mf_loss + self.emb_loss + self.reg_loss\n",
        "\n",
        "        self.opt = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss)\n",
        "\n",
        "    def _init_weights(self):\n",
        "        all_weights = dict()\n",
        "\n",
        "        initializer = tf.contrib.layers.xavier_initializer()\n",
        "\n",
        "        if self.pretrain_data is None:\n",
        "            all_weights['user_embedding'] = tf.Variable(initializer([self.n_users, self.emb_dim]), name='user_embedding')\n",
        "            all_weights['item_embedding'] = tf.Variable(initializer([self.n_items, self.emb_dim]), name='item_embedding')\n",
        "            print('using xavier initialization')\n",
        "        else:\n",
        "            all_weights['user_embedding'] = tf.Variable(initial_value=self.pretrain_data['user_embed'], trainable=True,\n",
        "                                                        name='user_embedding', dtype=tf.float32)\n",
        "            all_weights['item_embedding'] = tf.Variable(initial_value=self.pretrain_data['item_embed'], trainable=True,\n",
        "                                                        name='item_embedding', dtype=tf.float32)\n",
        "            print('using pretrained initialization')\n",
        "\n",
        "        self.weight_size_list = [self.emb_dim] + self.weight_size\n",
        "\n",
        "        for k in range(self.n_layers):\n",
        "            all_weights['W_gc_%d' %k] = tf.Variable(\n",
        "                initializer([self.weight_size_list[k], self.weight_size_list[k+1]]), name='W_gc_%d' % k)\n",
        "            all_weights['b_gc_%d' %k] = tf.Variable(\n",
        "                initializer([1, self.weight_size_list[k+1]]), name='b_gc_%d' % k)\n",
        "\n",
        "            all_weights['W_bi_%d' % k] = tf.Variable(\n",
        "                initializer([self.weight_size_list[k], self.weight_size_list[k + 1]]), name='W_bi_%d' % k)\n",
        "            all_weights['b_bi_%d' % k] = tf.Variable(\n",
        "                initializer([1, self.weight_size_list[k + 1]]), name='b_bi_%d' % k)\n",
        "\n",
        "            all_weights['W_mlp_%d' % k] = tf.Variable(\n",
        "                initializer([self.weight_size_list[k], self.weight_size_list[k+1]]), name='W_mlp_%d' % k)\n",
        "            all_weights['b_mlp_%d' % k] = tf.Variable(\n",
        "                initializer([1, self.weight_size_list[k+1]]), name='b_mlp_%d' % k)\n",
        "\n",
        "        return all_weights\n",
        "\n",
        "    def _split_A_hat(self, X):\n",
        "        A_fold_hat = []\n",
        "\n",
        "        fold_len = (self.n_users + self.n_items) // self.n_fold\n",
        "        for i_fold in range(self.n_fold):\n",
        "            start = i_fold * fold_len\n",
        "            if i_fold == self.n_fold -1:\n",
        "                end = self.n_users + self.n_items\n",
        "            else:\n",
        "                end = (i_fold + 1) * fold_len\n",
        "\n",
        "            A_fold_hat.append(self._convert_sp_mat_to_sp_tensor(X[start:end]))\n",
        "        return A_fold_hat\n",
        "\n",
        "    def _split_A_hat_node_dropout(self, X):\n",
        "        A_fold_hat = []\n",
        "\n",
        "        fold_len = (self.n_users + self.n_items) // self.n_fold\n",
        "        for i_fold in range(self.n_fold):\n",
        "            start = i_fold * fold_len\n",
        "            if i_fold == self.n_fold -1:\n",
        "                end = self.n_users + self.n_items\n",
        "            else:\n",
        "                end = (i_fold + 1) * fold_len\n",
        "\n",
        "            # A_fold_hat.append(self._convert_sp_mat_to_sp_tensor(X[start:end]))\n",
        "            temp = self._convert_sp_mat_to_sp_tensor(X[start:end])\n",
        "            n_nonzero_temp = X[start:end].count_nonzero()\n",
        "            A_fold_hat.append(self._dropout_sparse(temp, 1 - self.node_dropout[0], n_nonzero_temp))\n",
        "\n",
        "        return A_fold_hat\n",
        "\n",
        "    def _create_ngcf_embed(self):\n",
        "        # Generate a set of adjacency sub-matrix.\n",
        "        if self.node_dropout_flag:\n",
        "            # node dropout.\n",
        "            A_fold_hat = self._split_A_hat_node_dropout(self.norm_adj)\n",
        "        else:\n",
        "            A_fold_hat = self._split_A_hat(self.norm_adj)\n",
        "\n",
        "        ego_embeddings = tf.concat([self.weights['user_embedding'], self.weights['item_embedding']], axis=0)\n",
        "\n",
        "        all_embeddings = [ego_embeddings]\n",
        "\n",
        "        for k in range(0, self.n_layers):\n",
        "\n",
        "            temp_embed = []\n",
        "            for f in range(self.n_fold):\n",
        "                temp_embed.append(tf.sparse_tensor_dense_matmul(A_fold_hat[f], ego_embeddings))\n",
        "\n",
        "            # sum messages of neighbors.\n",
        "            side_embeddings = tf.concat(temp_embed, 0)\n",
        "            # transformed sum messages of neighbors.\n",
        "            sum_embeddings = tf.nn.leaky_relu(\n",
        "                tf.matmul(side_embeddings, self.weights['W_gc_%d' % k]) + self.weights['b_gc_%d' % k])\n",
        "\n",
        "            # bi messages of neighbors.\n",
        "            bi_embeddings = tf.multiply(ego_embeddings, side_embeddings)\n",
        "            # transformed bi messages of neighbors.\n",
        "            bi_embeddings = tf.nn.leaky_relu(\n",
        "                tf.matmul(bi_embeddings, self.weights['W_bi_%d' % k]) + self.weights['b_bi_%d' % k])\n",
        "\n",
        "            # non-linear activation.\n",
        "            ego_embeddings = sum_embeddings + bi_embeddings\n",
        "\n",
        "            # message dropout.\n",
        "            ego_embeddings = tf.nn.dropout(ego_embeddings, 1 - self.mess_dropout[k])\n",
        "\n",
        "            # normalize the distribution of embeddings.\n",
        "            norm_embeddings = tf.math.l2_normalize(ego_embeddings, axis=1)\n",
        "\n",
        "            all_embeddings += [norm_embeddings]\n",
        "\n",
        "        all_embeddings = tf.concat(all_embeddings, 1)\n",
        "        u_g_embeddings, i_g_embeddings = tf.split(all_embeddings, [self.n_users, self.n_items], 0)\n",
        "        return u_g_embeddings, i_g_embeddings\n",
        "\n",
        "    def _create_gcn_embed(self):\n",
        "        A_fold_hat = self._split_A_hat(self.norm_adj)\n",
        "        embeddings = tf.concat([self.weights['user_embedding'], self.weights['item_embedding']], axis=0)\n",
        "\n",
        "\n",
        "        all_embeddings = [embeddings]\n",
        "\n",
        "        for k in range(0, self.n_layers):\n",
        "            temp_embed = []\n",
        "            for f in range(self.n_fold):\n",
        "                temp_embed.append(tf.sparse_tensor_dense_matmul(A_fold_hat[f], embeddings))\n",
        "\n",
        "            embeddings = tf.concat(temp_embed, 0)\n",
        "            embeddings = tf.nn.leaky_relu(tf.matmul(embeddings, self.weights['W_gc_%d' %k]) + self.weights['b_gc_%d' %k])\n",
        "            embeddings = tf.nn.dropout(embeddings, 1 - self.mess_dropout[k])\n",
        "\n",
        "            all_embeddings += [embeddings]\n",
        "\n",
        "        all_embeddings = tf.concat(all_embeddings, 1)\n",
        "        u_g_embeddings, i_g_embeddings = tf.split(all_embeddings, [self.n_users, self.n_items], 0)\n",
        "        return u_g_embeddings, i_g_embeddings\n",
        "\n",
        "    def _create_gcmc_embed(self):\n",
        "        A_fold_hat = self._split_A_hat(self.norm_adj)\n",
        "\n",
        "        embeddings = tf.concat([self.weights['user_embedding'], self.weights['item_embedding']], axis=0)\n",
        "\n",
        "        all_embeddings = []\n",
        "\n",
        "        for k in range(0, self.n_layers):\n",
        "            temp_embed = []\n",
        "            for f in range(self.n_fold):\n",
        "                temp_embed.append(tf.sparse_tensor_dense_matmul(A_fold_hat[f], embeddings))\n",
        "            embeddings = tf.concat(temp_embed, 0)\n",
        "            # convolutional layer.\n",
        "            embeddings = tf.nn.leaky_relu(tf.matmul(embeddings, self.weights['W_gc_%d' % k]) + self.weights['b_gc_%d' % k])\n",
        "            # dense layer.\n",
        "            mlp_embeddings = tf.matmul(embeddings, self.weights['W_mlp_%d' %k]) + self.weights['b_mlp_%d' %k]\n",
        "            mlp_embeddings = tf.nn.dropout(mlp_embeddings, 1 - self.mess_dropout[k])\n",
        "\n",
        "            all_embeddings += [mlp_embeddings]\n",
        "        all_embeddings = tf.concat(all_embeddings, 1)\n",
        "\n",
        "        u_g_embeddings, i_g_embeddings = tf.split(all_embeddings, [self.n_users, self.n_items], 0)\n",
        "        return u_g_embeddings, i_g_embeddings\n",
        "\n",
        "\n",
        "    def create_bpr_loss(self, users, pos_items, neg_items):\n",
        "        pos_scores = tf.reduce_sum(tf.multiply(users, pos_items), axis=1)\n",
        "        neg_scores = tf.reduce_sum(tf.multiply(users, neg_items), axis=1)\n",
        "\n",
        "        regularizer = tf.nn.l2_loss(users) + tf.nn.l2_loss(pos_items) + tf.nn.l2_loss(neg_items)\n",
        "        regularizer = regularizer/self.batch_size\n",
        "        \n",
        "        # In the first version, we implement the bpr loss via the following codes:\n",
        "        # We report the performance in our paper using this implementation.\n",
        "        maxi = tf.log(tf.nn.sigmoid(pos_scores - neg_scores))\n",
        "        mf_loss = tf.negative(tf.reduce_mean(maxi))\n",
        "        \n",
        "        ## In the second version, we implement the bpr loss via the following codes to avoid 'NAN' loss during training:\n",
        "        ## However, it will change the training performance and training performance.\n",
        "        ## Please retrain the model and do a grid search for the best experimental setting.\n",
        "        # mf_loss = tf.reduce_sum(tf.nn.softplus(-(pos_scores - neg_scores)))\n",
        "        \n",
        "\n",
        "        emb_loss = self.decay * regularizer\n",
        "\n",
        "        reg_loss = tf.constant(0.0, tf.float32, [1])\n",
        "\n",
        "        return mf_loss, emb_loss, reg_loss\n",
        "\n",
        "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
        "        coo = X.tocoo().astype(np.float32)\n",
        "        indices = np.mat([coo.row, coo.col]).transpose()\n",
        "        return tf.SparseTensor(indices, coo.data, coo.shape)\n",
        "\n",
        "    def _dropout_sparse(self, X, keep_prob, n_nonzero_elems):\n",
        "        \"\"\"\n",
        "        Dropout for sparse tensors.\n",
        "        \"\"\"\n",
        "        noise_shape = [n_nonzero_elems]\n",
        "        random_tensor = keep_prob\n",
        "        random_tensor += tf.random_uniform(noise_shape)\n",
        "        dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
        "        pre_out = tf.sparse_retain(X, dropout_mask)\n",
        "\n",
        "        return pre_out * tf.div(1., keep_prob)\n",
        "\n",
        "def load_pretrained_data():\n",
        "    pretrain_path = '%spretrain/%s/%s.npz' % (args.proj_path, args.dataset, 'embedding')\n",
        "    try:\n",
        "        pretrain_data = np.load(pretrain_path)\n",
        "        print('load the pretrained embeddings.')\n",
        "    except Exception:\n",
        "        pretrain_data = None\n",
        "    return pretrain_data\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_id)\n",
        "\n",
        "    config = dict()\n",
        "    config['n_users'] = data_generator.n_users\n",
        "    config['n_items'] = data_generator.n_items\n",
        "\n",
        "    \"\"\"\n",
        "    *********************************************************\n",
        "    Generate the Laplacian matrix, where each entry defines the decay factor (e.g., p_ui) between two connected nodes.\n",
        "    \"\"\"\n",
        "    plain_adj, norm_adj, mean_adj = data_generator.get_adj_mat()\n",
        "\n",
        "    if args.adj_type == 'plain':\n",
        "        config['norm_adj'] = plain_adj\n",
        "        print('use the plain adjacency matrix')\n",
        "\n",
        "    elif args.adj_type == 'norm':\n",
        "        config['norm_adj'] = norm_adj\n",
        "        print('use the normalized adjacency matrix')\n",
        "\n",
        "    elif args.adj_type == 'gcmc':\n",
        "        config['norm_adj'] = mean_adj\n",
        "        print('use the gcmc adjacency matrix')\n",
        "\n",
        "    else:\n",
        "        config['norm_adj'] = mean_adj + sp.eye(mean_adj.shape[0])\n",
        "        print('use the mean adjacency matrix')\n",
        "\n",
        "    t0 = time()\n",
        "\n",
        "    if args.pretrain == -1:\n",
        "        pretrain_data = load_pretrained_data()\n",
        "    else:\n",
        "        pretrain_data = None\n",
        "\n",
        "    model = NGCF(data_config=config, pretrain_data=pretrain_data)\n",
        "\n",
        "    \"\"\"\n",
        "    *********************************************************\n",
        "    Save the model parameters.\n",
        "    \"\"\"\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    if args.save_flag == 1:\n",
        "        layer = '-'.join([str(l) for l in eval(args.layer_size)])\n",
        "        weights_save_path = '%sweights/%s/%s/%s/l%s_r%s' % (args.weights_path, args.dataset, model.model_type, layer,\n",
        "                                                            str(args.lr), '-'.join([str(r) for r in eval(args.regs)]))\n",
        "        ensureDir(weights_save_path)\n",
        "        save_saver = tf.train.Saver(max_to_keep=1)\n",
        "\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    sess = tf.Session(config=config)\n",
        "\n",
        "    \"\"\"\n",
        "    *********************************************************\n",
        "    Reload the pretrained model parameters.\n",
        "    \"\"\"\n",
        "    if args.pretrain == 1:\n",
        "        layer = '-'.join([str(l) for l in eval(args.layer_size)])\n",
        "\n",
        "        pretrain_path = '%sweights/%s/%s/%s/l%s_r%s' % (args.weights_path, args.dataset, model.model_type, layer,\n",
        "                                                        str(args.lr), '-'.join([str(r) for r in eval(args.regs)]))\n",
        "\n",
        "\n",
        "        ckpt = tf.train.get_checkpoint_state(os.path.dirname(pretrain_path + '/checkpoint'))\n",
        "        if ckpt and ckpt.model_checkpoint_path:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "            print('load the pretrained model parameters from: ', pretrain_path)\n",
        "\n",
        "            # *********************************************************\n",
        "            # get the performance from pretrained model.\n",
        "            if args.report != 1:\n",
        "                users_to_test = list(data_generator.test_set.keys())\n",
        "                ret = test(sess, model, users_to_test, drop_flag=True)\n",
        "                cur_best_pre_0 = ret['recall'][0]\n",
        "\n",
        "                pretrain_ret = 'pretrained model recall=[%.5f, %.5f], precision=[%.5f, %.5f], hit=[%.5f, %.5f],' \\\n",
        "                               'ndcg=[%.5f, %.5f]' % \\\n",
        "                               (ret['recall'][0], ret['recall'][-1],\n",
        "                                ret['precision'][0], ret['precision'][-1],\n",
        "                                ret['hit_ratio'][0], ret['hit_ratio'][-1],\n",
        "                                ret['ndcg'][0], ret['ndcg'][-1])\n",
        "                print(pretrain_ret)\n",
        "        else:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            cur_best_pre_0 = 0.\n",
        "            print('without pretraining.')\n",
        "\n",
        "    else:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        cur_best_pre_0 = 0.\n",
        "        print('without pretraining.')\n",
        "\n",
        "    \"\"\"\n",
        "    *********************************************************\n",
        "    Get the performance w.r.t. different sparsity levels.\n",
        "    \"\"\"\n",
        "    if args.report == 1:\n",
        "        assert args.test_flag == 'full'\n",
        "        users_to_test_list, split_state = data_generator.get_sparsity_split()\n",
        "        users_to_test_list.append(list(data_generator.test_set.keys()))\n",
        "        split_state.append('all')\n",
        "\n",
        "        report_path = '%sreport/%s/%s.result' % (args.proj_path, args.dataset, model.model_type)\n",
        "        ensureDir(report_path)\n",
        "        f = open(report_path, 'w')\n",
        "        f.write(\n",
        "            'embed_size=%d, lr=%.4f, layer_size=%s, keep_prob=%s, regs=%s, loss_type=%s, adj_type=%s\\n'\n",
        "            % (args.embed_size, args.lr, args.layer_size, args.keep_prob, args.regs, args.loss_type, args.adj_type))\n",
        "\n",
        "        for i, users_to_test in enumerate(users_to_test_list):\n",
        "            ret = test(sess, model, users_to_test, drop_flag=True)\n",
        "\n",
        "            final_perf = \"recall=[%s], precision=[%s], hit=[%s], ndcg=[%s]\" % \\\n",
        "                         ('\\t'.join(['%.5f' % r for r in ret['recall']]),\n",
        "                          '\\t'.join(['%.5f' % r for r in ret['precision']]),\n",
        "                          '\\t'.join(['%.5f' % r for r in ret['hit_ratio']]),\n",
        "                          '\\t'.join(['%.5f' % r for r in ret['ndcg']]))\n",
        "            print(final_perf)\n",
        "\n",
        "            f.write('\\t%s\\n\\t%s\\n' % (split_state[i], final_perf))\n",
        "        f.close()\n",
        "        exit()\n",
        "\n",
        "    \"\"\"\n",
        "    *********************************************************\n",
        "    Train.\n",
        "    \"\"\"\n",
        "    loss_loger, pre_loger, rec_loger, ndcg_loger, hit_loger = [], [], [], [], []\n",
        "    stopping_step = 0\n",
        "    should_stop = False\n",
        "\n",
        "    for epoch in range(args.epoch):\n",
        "        t1 = time()\n",
        "        loss, mf_loss, emb_loss, reg_loss = 0., 0., 0., 0.\n",
        "        n_batch = data_generator.n_train // args.batch_size + 1\n",
        "\n",
        "        for idx in range(n_batch):\n",
        "            users, pos_items, neg_items = data_generator.sample()\n",
        "            _, batch_loss, batch_mf_loss, batch_emb_loss, batch_reg_loss = sess.run([model.opt, model.loss, model.mf_loss, model.emb_loss, model.reg_loss],\n",
        "                               feed_dict={model.users: users, model.pos_items: pos_items,\n",
        "                                          model.node_dropout: eval(args.node_dropout),\n",
        "                                          model.mess_dropout: eval(args.mess_dropout),\n",
        "                                          model.neg_items: neg_items})\n",
        "            loss += batch_loss\n",
        "            mf_loss += batch_mf_loss\n",
        "            emb_loss += batch_emb_loss\n",
        "            reg_loss += batch_reg_loss\n",
        "\n",
        "        if np.isnan(loss) == True:\n",
        "            print('ERROR: loss is nan.')\n",
        "            sys.exit()\n",
        "\n",
        "        # print the test evaluation metrics each 10 epochs; pos:neg = 1:10.\n",
        "        if (epoch + 1) % 10 != 0:\n",
        "            if args.verbose > 0 and epoch % args.verbose == 0:\n",
        "                perf_str = 'Epoch %d [%.1fs]: train==[%.5f=%.5f + %.5f]' % (\n",
        "                    epoch, time() - t1, loss, mf_loss, reg_loss)\n",
        "                print(perf_str)\n",
        "            continue\n",
        "\n",
        "        t2 = time()\n",
        "        users_to_test = list(data_generator.test_set.keys())\n",
        "        ret = test(sess, model, users_to_test, drop_flag=True)\n",
        "\n",
        "        t3 = time()\n",
        "\n",
        "        loss_loger.append(loss)\n",
        "        rec_loger.append(ret['recall'])\n",
        "        pre_loger.append(ret['precision'])\n",
        "        ndcg_loger.append(ret['ndcg'])\n",
        "        hit_loger.append(ret['hit_ratio'])\n",
        "\n",
        "        if args.verbose > 0:\n",
        "            perf_str = 'Epoch %d [%.1fs + %.1fs]: train==[%.5f=%.5f + %.5f + %.5f], recall=[%.5f, %.5f], ' \\\n",
        "                       'precision=[%.5f, %.5f], hit=[%.5f, %.5f], ndcg=[%.5f, %.5f]' % \\\n",
        "                       (epoch, t2 - t1, t3 - t2, loss, mf_loss, emb_loss, reg_loss, ret['recall'][0], ret['recall'][-1],\n",
        "                        ret['precision'][0], ret['precision'][-1], ret['hit_ratio'][0], ret['hit_ratio'][-1],\n",
        "                        ret['ndcg'][0], ret['ndcg'][-1])\n",
        "            print(perf_str)\n",
        "\n",
        "        cur_best_pre_0, stopping_step, should_stop = early_stopping(ret['recall'][0], cur_best_pre_0,\n",
        "                                                                    stopping_step, expected_order='acc', flag_step=5)\n",
        "\n",
        "        # *********************************************************\n",
        "        # early stopping when cur_best_pre_0 is decreasing for ten successive steps.\n",
        "        if should_stop == True:\n",
        "            break\n",
        "\n",
        "        # *********************************************************\n",
        "        # save the user & item embeddings for pretraining.\n",
        "        if ret['recall'][0] == cur_best_pre_0 and args.save_flag == 1:\n",
        "            save_saver.save(sess, weights_save_path + '/weights', global_step=epoch)\n",
        "            print('save the weights in path: ', weights_save_path)\n",
        "\n",
        "    recs = np.array(rec_loger)\n",
        "    pres = np.array(pre_loger)\n",
        "    ndcgs = np.array(ndcg_loger)\n",
        "    hit = np.array(hit_loger)\n",
        "\n",
        "    best_rec_0 = max(recs[:, 0])\n",
        "    idx = list(recs[:, 0]).index(best_rec_0)\n",
        "\n",
        "    final_perf = \"Best Iter=[%d]@[%.1f]\\trecall=[%s], precision=[%s], hit=[%s], ndcg=[%s]\" % \\\n",
        "                 (idx, time() - t0, '\\t'.join(['%.5f' % r for r in recs[idx]]),\n",
        "                  '\\t'.join(['%.5f' % r for r in pres[idx]]),\n",
        "                  '\\t'.join(['%.5f' % r for r in hit[idx]]),\n",
        "                  '\\t'.join(['%.5f' % r for r in ndcgs[idx]]))\n",
        "    print(final_perf)\n",
        "\n",
        "    save_path = '%soutput/%s/%s.result' % (args.proj_path, args.dataset, model.model_type)\n",
        "    ensureDir(save_path)\n",
        "    f = open(save_path, 'a')\n",
        "\n",
        "    f.write(\n",
        "        'embed_size=%d, lr=%.4f, layer_size=%s, node_dropout=%s, mess_dropout=%s, regs=%s, adj_type=%s\\n\\t%s\\n'\n",
        "        % (args.embed_size, args.lr, args.layer_size, args.node_dropout, args.mess_dropout, args.regs,\n",
        "           args.adj_type, final_perf))\n",
        "    f.close()"
      ]
    }
  ]
}