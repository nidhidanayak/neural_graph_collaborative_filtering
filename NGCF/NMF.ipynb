{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8S2rFvXJPdfo"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Created on Oct 10, 2018\n",
        "Tensorflow Implementation of the baseline of \"Neural Collaborative Filtering, He et al. SIGIR 2017\" in:\n",
        "Wang Xiang et al. Neural Graph Collaborative Filtering. In SIGIR 2019.\n",
        "@author: Xiang Wang (xiangwang@u.nus.edu)\n",
        "'''\n",
        "import tensorflow as tf\n",
        "from utility.helper import *\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "from utility.batch_test import *\n",
        "import os\n",
        "import sys\n",
        "from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
        "\n",
        "class NMF(object):\n",
        "    def __init__(self, data_config, pretrain_data):\n",
        "        self.model_type = 'nmf'\n",
        "        self.pretrain_data = pretrain_data\n",
        "\n",
        "        self.n_users = data_config['n_users']\n",
        "        self.n_items = data_config['n_items']\n",
        "\n",
        "        self.lr = args.lr\n",
        "        # self.lr_decay = args.lr_decay\n",
        "\n",
        "        self.emb_dim = args.embed_size\n",
        "        self.batch_size = args.batch_size\n",
        "\n",
        "        self.weight_size = eval(args.layer_size)\n",
        "        self.n_layers = len(self.weight_size)\n",
        "\n",
        "        self.model_type += '_l%d' % self.n_layers\n",
        "\n",
        "        self.regs = eval(args.regs)\n",
        "        self.decay = self.regs[-1]\n",
        "\n",
        "        self.verbose = args.verbose\n",
        "\n",
        "        # placeholder definition\n",
        "        self.users = tf.placeholder(tf.int32, shape=(None))\n",
        "        self.pos_items = tf.placeholder(tf.int32, shape=(None))\n",
        "        self.neg_items = tf.placeholder(tf.int32, shape=(None))\n",
        "\n",
        "        self.dropout_keep = tf.placeholder(tf.float32, shape=[None])\n",
        "        self.train_phase = tf.placeholder(tf.bool)\n",
        "\n",
        "        # self.global_step = tf.Variable(0, trainable=False)\n",
        "        self.weights = self._init_weights()\n",
        "\n",
        "\n",
        "        # Original embedding.\n",
        "        u_e = tf.nn.embedding_lookup(self.weights['user_embedding'], self.users)\n",
        "        pos_i_e = tf.nn.embedding_lookup(self.weights['item_embedding'], self.pos_items)\n",
        "        neg_i_e = tf.nn.embedding_lookup(self.weights['item_embedding'], self.neg_items)\n",
        "\n",
        "        # All ratings for all users.\n",
        "        self.batch_ratings = self._create_batch_ratings(u_e, pos_i_e)\n",
        "\n",
        "        self.mf_loss, self.emb_loss, self.reg_loss = self.create_bpr_loss(u_e, pos_i_e, neg_i_e)\n",
        "        self.loss = self.mf_loss + self.emb_loss + self.reg_loss\n",
        "\n",
        "        # self.dy_lr = tf.train.exponential_decay(self.lr, self.global_step, 10000, self.lr_decay, staircase=True)\n",
        "        # self.opt = tf.train.RMSPropOptimizer(learning_rate=self.dy_lr).minimize(self.loss, global_step=self.global_step)\n",
        "        self.opt = tf.train.RMSPropOptimizer(learning_rate=self.lr).minimize(self.loss)\n",
        "        # self.updates = self.opt.minimize(self.loss, var_list=self.weights)\n",
        "\n",
        "        self._statistics_params()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        all_weights = dict()\n",
        "\n",
        "        initializer = tf.contrib.layers.xavier_initializer()\n",
        "\n",
        "\n",
        "        if self.pretrain_data is None:\n",
        "            all_weights['user_embedding'] = tf.Variable(initializer([self.n_users, self.emb_dim]), name='user_embedding')\n",
        "            all_weights['item_embedding'] = tf.Variable(initializer([self.n_items, self.emb_dim]), name='item_embedding')\n",
        "            print('using xavier initialization')\n",
        "        else:\n",
        "            all_weights['user_embedding'] = tf.Variable(initial_value=self.pretrain_data['user_embed'], trainable=True,\n",
        "                                                        name='user_embedding', dtype=tf.float32)\n",
        "            all_weights['item_embedding'] = tf.Variable(initial_value=self.pretrain_data['item_embed'], trainable=True,\n",
        "                                                        name='item_embedding', dtype=tf.float32)\n",
        "            print('using pretrained initialization')\n",
        "\n",
        "\n",
        "        if self.model_type == 'mlp':\n",
        "            self.weight_size_list = [2 * self.emb_dim] + self.weight_size\n",
        "        elif self.model_type == 'jrl':\n",
        "            self.weight_size_list = [self.emb_dim] + self.weight_size\n",
        "        else:\n",
        "            self.weight_size_list = [3 * self.emb_dim] + self.weight_size\n",
        "\n",
        "        for i in range(self.n_layers):\n",
        "            all_weights['W_%d' %i] = tf.Variable(\n",
        "                initializer([self.weight_size_list[i], self.weight_size_list[i+1]]), name='W_%d' %i)\n",
        "            all_weights['b_%d' %i] = tf.Variable(\n",
        "                initializer([1, self.weight_size_list[i+1]]), name='b_%d' %i)\n",
        "\n",
        "        all_weights['h'] = tf.Variable(initializer([self.weight_size_list[-1], 1]), name='h')\n",
        "\n",
        "        return all_weights\n",
        "\n",
        "    def create_bpr_loss(self, users, pos_items, neg_items):\n",
        "        pos_scores = self._create_inference(users, pos_items)\n",
        "        neg_scores = self._create_inference(users, neg_items)\n",
        "\n",
        "        regularizer = tf.nn.l2_loss(users) + tf.nn.l2_loss(pos_items) + tf.nn.l2_loss(neg_items)\n",
        "        regularizer = regularizer/self.batch_size\n",
        "\n",
        "        maxi = tf.log(tf.nn.sigmoid(pos_scores - neg_scores))\n",
        "        mf_loss = tf.negative(tf.reduce_mean(maxi))\n",
        "\n",
        "        emb_loss = self.regs[-1] * regularizer\n",
        "\n",
        "        reg_loss = self.regs[-2] * tf.nn.l2_loss(self.weights['h'])\n",
        "\n",
        "        return mf_loss, emb_loss, reg_loss\n",
        "\n",
        "    def _create_inference(self, u_e, i_e):\n",
        "        z = []\n",
        "\n",
        "        if self.model_type == 'mlp':\n",
        "            z.append(tf.concat([u_e, i_e], 1))\n",
        "        elif self.model_type == 'jrl':\n",
        "            z.append(u_e * i_e)\n",
        "        else:\n",
        "            z.append(tf.concat([u_e, i_e, u_e * i_e], 1))\n",
        "\n",
        "        # z[0] = self.batch_norm_layer(z[0], train_phase=self.train_phase, scope_bn='bn_mlp')\n",
        "\n",
        "        for i in range(self.n_layers):\n",
        "            # (batch, W[i]) * (W[i], W[i+1]) + (1, W[i+1]) => (batch, W[i+1])\n",
        "            # temp = self.batch_norm_layer(z[i], train_phase=self.train_phase, scope_bn='mlp_%d' % i)\n",
        "\n",
        "            temp = tf.nn.relu(tf.matmul(z[i], self.weights['W_%d' % i]) + self.weights['b_%d' % i])\n",
        "            temp = tf.nn.dropout(temp, self.dropout_keep[i])\n",
        "            z.append(temp)\n",
        "\n",
        "        agg_out = tf.matmul(z[-1], self.weights['h'])\n",
        "        return agg_out\n",
        "\n",
        "    def _create_all_ratings(self, u_e):\n",
        "        z = []\n",
        "\n",
        "        if self.model_type == 'jrl':\n",
        "            u_1 = tf.expand_dims(u_e, axis=1)\n",
        "            i_1 = tf.expand_dims(self.weights['item_embedding'], axis=0)\n",
        "            z.append(tf.reshape(u_1 * i_1, [-1, self.emb_dim]))\n",
        "\n",
        "        elif self.model_type == 'mlp':\n",
        "            u_1 = tf.reshape(tf.tile(u_e, [1, self.n_items]), [-1, self.emb_dim])\n",
        "            i_1 = tf.tile(self.weights['item_embedding'], [self.batch_size, 1])\n",
        "            z.append(tf.concat([u_1, i_1], 1))\n",
        "        else:\n",
        "            u_1 = tf.expand_dims(u_e, axis=1)\n",
        "            i_1 = tf.expand_dims(self.weights['item_embedding'], axis=0)\n",
        "            u_i = tf.reshape(u_1 * i_1, [-1, self.emb_dim])\n",
        "\n",
        "            u_1 = tf.reshape(tf.tile(u_e, [1, self.n_items]), [-1, self.emb_dim])\n",
        "            i_1 = tf.tile(self.weights['item_embedding'], [self.batch_size, 1])\n",
        "            z.append(tf.concat([u_1, i_1, u_i], 1))\n",
        "\n",
        "        for i in range(self.n_layers):\n",
        "            # (batch, W[i]) * (W[i], W[i+1]) + (1, W[i+1]) => (batch, W[i+1])\n",
        "            z.append(tf.nn.relu(tf.matmul(z[i], self.weights['W_%d' % i]) + self.weights['b_%d' % i]))\n",
        "\n",
        "        agg_out = tf.matmul(z[-1], self.weights['h']) # (batch, W[-1]) * (W[-1], 1) => (batch, 1)\n",
        "        all_ratings = tf.reshape(agg_out, [-1, self.n_items])\n",
        "        return all_ratings\n",
        "\n",
        "    def _create_batch_ratings(self, u_e, i_e):\n",
        "        z = []\n",
        "\n",
        "        n_b_user = tf.shape(u_e)[0]\n",
        "        n_b_item = tf.shape(i_e)[0]\n",
        "\n",
        "\n",
        "        if self.model_type == 'jrl':\n",
        "            u_1 = tf.expand_dims(u_e, axis=1)\n",
        "            i_1 = tf.expand_dims(i_e, axis=0)\n",
        "            z.append(tf.reshape(u_1 * i_1, [-1, self.emb_dim])) # (n_b_user * n_b_item, embed_size)\n",
        "\n",
        "        elif self.model_type == 'mlp':\n",
        "            u_1 = tf.reshape(tf.tile(u_e, [1, n_b_item]), [-1, self.emb_dim])\n",
        "            i_1 = tf.tile(i_e, [n_b_user, 1])\n",
        "            z.append(tf.concat([u_1, i_1], 1)) # (n_b_user * n_b_item, 2*embed_size)\n",
        "        else:\n",
        "            u_1 = tf.expand_dims(u_e, axis=1)\n",
        "            i_1 = tf.expand_dims(i_e, axis=0)\n",
        "            u_i = tf.reshape(u_1 * i_1, [-1, self.emb_dim])\n",
        "\n",
        "            u_1 = tf.reshape(tf.tile(u_e, [1, n_b_item]), [-1, self.emb_dim])\n",
        "            i_1 = tf.tile(i_e, [n_b_user, 1])\n",
        "            z.append(tf.concat([u_1, i_1, u_i], 1))\n",
        "\n",
        "        for i in range(self.n_layers):\n",
        "            # (batch, W[i]) * (W[i], W[i+1]) + (1, W[i+1]) => (batch, W[i+1])\n",
        "            z.append(tf.nn.relu(tf.matmul(z[i], self.weights['W_%d' % i]) + self.weights['b_%d' % i]))\n",
        "\n",
        "        agg_out = tf.matmul(z[-1], self.weights['h']) # (batch, W[-1]) * (W[-1], 1) => (batch, 1)\n",
        "        batch_ratings = tf.reshape(agg_out, [n_b_user, n_b_item])\n",
        "        return batch_ratings\n",
        "\n",
        "    def batch_norm_layer(self, x, train_phase, scope_bn):\n",
        "        with tf.variable_scope(scope_bn):\n",
        "            return batch_norm(x, decay=0.9, center=True, scale=True, updates_collections=None,\n",
        "            is_training=True, reuse=tf.AUTO_REUSE, trainable=True, scope=scope_bn)\n",
        "\n",
        "    def _statistics_params(self):\n",
        "        # number of params\n",
        "        total_parameters = 0\n",
        "        for variable in self.weights.values():\n",
        "            shape = variable.get_shape()  # shape is an array of tf.Dimension\n",
        "            variable_parameters = 1\n",
        "            for dim in shape:\n",
        "                variable_parameters *= dim.value\n",
        "            total_parameters += variable_parameters\n",
        "        if self.verbose > 0:\n",
        "            print(\"#params: %d\" % total_parameters)\n",
        "\n",
        "def load_pretrained_data():\n",
        "    pretrain_path = '%spretrain/%s/%s.npz' % (args.proj_path, args.dataset, 'bprmf')\n",
        "    pretrain_data = np.load(pretrain_path)\n",
        "    print('load the pretrained bprmf model parameters.')\n",
        "    return pretrain_data\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_id)\n",
        "\n",
        "    config = dict()\n",
        "    config['n_users'] = data_generator.n_users\n",
        "    config['n_items'] = data_generator.n_items\n",
        "\n",
        "    t0 = time()\n",
        "\n",
        "    if args.pretrain == -1:\n",
        "        pretrain_data = load_pretrained_data()\n",
        "    else:\n",
        "        pretrain_data = None\n",
        "\n",
        "\n",
        "    model = NMF(data_config=config, pretrain_data=pretrain_data)\n",
        "\n",
        "    saver = tf.train.Saver()\n",
        "    # *********************************************************\n",
        "    # save the model parameters.\n",
        "    if args.save_flag == 1:\n",
        "        layer = '-'.join([str(l) for l in eval(args.layer_size)])\n",
        "        weights_save_path = '%sweights/%s/%s/%s/l%s_r%s' % (args.proj_path, args.dataset, model.model_type, layer, str(args.lr),\n",
        "                                                            '-'.join([str(r) for r in eval(args.regs)]))\n",
        "        ensureDir(weights_save_path)\n",
        "        save_saver = tf.train.Saver(max_to_keep=1)\n",
        "\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    sess = tf.Session(config=config)\n",
        "\n",
        "    # *********************************************************\n",
        "    # reload the pretrained model parameters.\n",
        "    if args.pretrain == 1:\n",
        "        layer = '-'.join([str(l) for l in eval(args.layer_size)])\n",
        "        pretrain_path = '%sweights/%s/%s/%s/l%s_r%s' % (args.proj_path, args.dataset, model.model_type, layer, str(args.lr),\n",
        "                                                    '-'.join([str(r) for r in eval(args.regs)]))\n",
        "        ckpt = tf.train.get_checkpoint_state(os.path.dirname(pretrain_path + '/checkpoint'))\n",
        "        if ckpt and ckpt.model_checkpoint_path:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "            print('load the pretrained model parameters from: ', pretrain_path)\n",
        "\n",
        "            # *********************************************************\n",
        "            # get the performance from pretrained model.\n",
        "            users_to_test = list(data_generator.test_set.keys())\n",
        "            ret = test(sess, model, users_to_test, drop_flag=True)\n",
        "            cur_best_pre_0 = ret['recall'][0]\n",
        "\n",
        "            pretrain_ret = 'pretrained model recall=[%.5f, %.5f], precision=[%.5f, %.5f], hit=[%.5f, %.5f],' \\\n",
        "                           'ndcg=[%.5f, %.5f], auc=[%.5f]' % \\\n",
        "                           (ret['recall'][0], ret['recall'][-1],\n",
        "                            ret['precision'][0], ret['precision'][-1],\n",
        "                            ret['hit_ratio'][0], ret['hit_ratio'][-1],\n",
        "                            ret['ndcg'][0], ret['ndcg'][-1], ret['auc'])\n",
        "            print(pretrain_ret)\n",
        "        else:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            cur_best_pre_0 = 0.\n",
        "            print('without pretraining.')\n",
        "\n",
        "    else:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        cur_best_pre_0 = 0.\n",
        "        print('without pretraining.')\n",
        "\n",
        "    loss_loger, pre_loger, rec_loger, ndcg_loger, auc_loger, hit_loger = [], [], [], [], [], []\n",
        "    stopping_step = 0\n",
        "    should_stop = False\n",
        "\n",
        "    for epoch in range(args.epoch):\n",
        "        t1 = time()\n",
        "        loss, mf_loss, emb_loss, reg_loss = 0., 0., 0., 0.\n",
        "        n_batch = data_generator.n_train // args.batch_size + 1\n",
        "\n",
        "\n",
        "        for idx in range(n_batch):\n",
        "            users, pos_items, neg_items = data_generator.sample()\n",
        "            _, batch_loss, batch_mf_loss, batch_emb_loss, batch_reg_loss = sess.run([model.opt, model.loss, model.mf_loss, model.emb_loss, model.reg_loss],\n",
        "                               feed_dict={model.users: users, model.pos_items: pos_items,\n",
        "                                          model.neg_items: neg_items,\n",
        "                                          model.dropout_keep: eval(args.keep_prob),\n",
        "                                          model.train_phase: True})\n",
        "            loss += batch_loss\n",
        "            mf_loss += batch_mf_loss\n",
        "            emb_loss += batch_emb_loss\n",
        "            reg_loss += batch_reg_loss\n",
        "\n",
        "        if np.isnan(loss) == True:\n",
        "            print('ERROR: loss is nan.')\n",
        "            sys.exit()\n",
        "\n",
        "        # print the test evaluation metrics each 10 epochs; pos:neg = 1:10.\n",
        "        if (epoch + 1) % 10 != 0:\n",
        "            if args.verbose > 0 and epoch % args.verbose == 0:\n",
        "                perf_str = 'Epoch %d [%.1fs]: train==[%.5f=%.5f + %.5f]' % (epoch, time() - t1, loss, mf_loss, reg_loss)\n",
        "                print(perf_str)\n",
        "            continue\n",
        "\n",
        "        t2 = time()\n",
        "        users_to_test = list(data_generator.test_set.keys())\n",
        "        ret = test(sess, model, users_to_test, batch_test_flag=True)\n",
        "\n",
        "        t3 = time()\n",
        "\n",
        "        loss_loger.append(loss)\n",
        "        rec_loger.append(ret['recall'])\n",
        "        pre_loger.append(ret['precision'])\n",
        "        ndcg_loger.append(ret['ndcg'])\n",
        "        auc_loger.append(ret['auc'])\n",
        "        hit_loger.append(ret['hit_ratio'])\n",
        "\n",
        "        if args.verbose > 0:\n",
        "            perf_str = 'Epoch %d [%.1fs + %.1fs]: train==[%.5f=%.5f + %.5f + %.5f], recall=[%.5f, %.5f], ' \\\n",
        "                       'precision=[%.5f, %.5f], hit=[%.5f, %.5f], ndcg=[%.5f, %.5f], auc=[%.5f]' % \\\n",
        "                       (epoch, t2 - t1, t3 - t2, loss, mf_loss, emb_loss, reg_loss, ret['recall'][0], ret['recall'][-1],\n",
        "                        ret['precision'][0], ret['precision'][-1], ret['hit_ratio'][0], ret['hit_ratio'][-1],\n",
        "                        ret['ndcg'][0], ret['ndcg'][-1], ret['auc'])\n",
        "            print(perf_str)\n",
        "\n",
        "        cur_best_pre_0, stopping_step, should_stop = early_stopping(ret['recall'][0], cur_best_pre_0,\n",
        "                                                                    stopping_step, expected_order='acc', flag_step=5)\n",
        "\n",
        "        # *********************************************************\n",
        "        # early stopping when cur_best_pre_0 is decreasing for ten successive steps.\n",
        "        if should_stop == True:\n",
        "            break\n",
        "\n",
        "        # *********************************************************\n",
        "        # save the user & item embeddings for pretraining.\n",
        "        if ret['recall'][0] == cur_best_pre_0 and args.save_flag == 1:\n",
        "            save_saver.save(sess, weights_save_path + '/weights', global_step=epoch)\n",
        "            print('save the weights in path: ', weights_save_path)\n",
        "\n",
        "    recs = np.array(rec_loger)\n",
        "    pres = np.array(pre_loger)\n",
        "    ndcgs = np.array(ndcg_loger)\n",
        "    auc = np.array(auc_loger)\n",
        "    hit = np.array(hit_loger)\n",
        "\n",
        "    best_rec_0 = max(recs[:, 0])\n",
        "    idx = list(recs[:, 0]).index(best_rec_0)\n",
        "\n",
        "    final_perf = \"Best Iter=[%d]@[%.1f]\\trecall=[%s], precision=[%s], hit=[%s], ndcg=[%s], auc=[%.5f]\" % \\\n",
        "                 (idx, time() - t0, '\\t'.join(['%.5f' % r for r in recs[idx]]),\n",
        "                  '\\t'.join(['%.5f' % r for r in pres[idx]]),\n",
        "                  '\\t'.join(['%.5f' % r for r in hit[idx]]),\n",
        "                  '\\t'.join(['%.5f' % r for r in ndcgs[idx]]),\n",
        "                  auc[idx])\n",
        "    print(final_perf)\n",
        "\n",
        "    save_path = '%soutput/%s/%s.result' % (args.proj_path, args.dataset, model.model_type)\n",
        "    ensureDir(save_path)\n",
        "    f = open(save_path, 'a')\n",
        "\n",
        "    f.write('embed_size=%d, lr=%.4f, layer_size=%s, keep_prob=%s, regs=%s, loss_type=%s, \\n\\t%s\\n'\n",
        "            % (args.embed_size, args.lr, args.layer_size, args.keep_prob, args.regs, args.loss_type, final_perf))\n",
        "    f.close()"
      ]
    }
  ]
}