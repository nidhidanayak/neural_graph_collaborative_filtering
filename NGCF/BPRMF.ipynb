{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BPRMF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8S2rFvXJPdfo"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Created on Oct 10, 2018\n",
        "Tensorflow Implementation of the baseline of \"Matrix Factorization with BPR Loss\" in:\n",
        "Wang Xiang et al. Neural Graph Collaborative Filtering. In SIGIR 2019.\n",
        "@author: Xiang Wang (xiangwang@u.nus.edu)\n",
        "'''\n",
        "import tensorflow as tf\n",
        "from utility.helper import *\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "from utility.batch_test import *\n",
        "import os\n",
        "import sys\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
        "\n",
        "class BPRMF(object):\n",
        "    def __init__(self, data_config):\n",
        "        self.model_type = 'bprmf'\n",
        "\n",
        "        self.n_users = data_config['n_users']\n",
        "        self.n_items = data_config['n_items']\n",
        "\n",
        "        self.lr = args.lr\n",
        "        # self.lr_decay = args.lr_decay\n",
        "\n",
        "        self.emb_dim = args.embed_size\n",
        "        self.batch_size = args.batch_size\n",
        "\n",
        "        self.weight_size = eval(args.layer_size)\n",
        "        self.n_layers = len(self.weight_size)\n",
        "\n",
        "        self.regs = eval(args.regs)\n",
        "        self.decay = self.regs[0]\n",
        "\n",
        "        self.verbose = args.verbose\n",
        "\n",
        "        # placeholder definition\n",
        "        self.users = tf.placeholder(tf.int32, shape=(None,))\n",
        "        self.pos_items = tf.placeholder(tf.int32, shape=(None,))\n",
        "        self.neg_items = tf.placeholder(tf.int32, shape=(None,))\n",
        "\n",
        "        # self.global_step = tf.Variable(0, trainable=False)\n",
        "\n",
        "        self.weights = self._init_weights()\n",
        "\n",
        "        # Original embedding.\n",
        "        u_e = tf.nn.embedding_lookup(self.weights['user_embedding'], self.users)\n",
        "        pos_i_e = tf.nn.embedding_lookup(self.weights['item_embedding'], self.pos_items)\n",
        "        neg_i_e = tf.nn.embedding_lookup(self.weights['item_embedding'], self.neg_items)\n",
        "\n",
        "        # All ratings for all users.\n",
        "        self.batch_ratings = tf.matmul(u_e, pos_i_e, transpose_a=False, transpose_b=True)\n",
        "\n",
        "        self.mf_loss, self.reg_loss = self.create_bpr_loss(u_e, pos_i_e, neg_i_e)\n",
        "        self.loss = self.mf_loss + self.reg_loss\n",
        "\n",
        "        # self.dy_lr = tf.train.exponential_decay(self.lr, self.global_step, 10000, self.lr_decay, staircase=True)\n",
        "        self.opt = tf.train.RMSPropOptimizer(learning_rate=self.lr).minimize(self.loss)\n",
        "\n",
        "        # self.updates = self.opt.minimize(self.loss, var_list=self.weights)\n",
        "\n",
        "        self._statistics_params()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        all_weights = dict()\n",
        "\n",
        "        initializer = tf.contrib.layers.xavier_initializer()\n",
        "\n",
        "\n",
        "        all_weights['user_embedding'] = tf.Variable(initializer([self.n_users, self.emb_dim]), name='user_embedding')\n",
        "        all_weights['item_embedding'] = tf.Variable(initializer([self.n_items, self.emb_dim]), name='item_embedding')\n",
        "\n",
        "        return all_weights\n",
        "\n",
        "    def create_bpr_loss(self, users, pos_items, neg_items):\n",
        "        pos_scores = tf.reduce_sum(tf.multiply(users, pos_items), axis=1)\n",
        "        neg_scores = tf.reduce_sum(tf.multiply(users, neg_items), axis=1)\n",
        "\n",
        "        regularizer = tf.nn.l2_loss(users) + tf.nn.l2_loss(pos_items) + tf.nn.l2_loss(neg_items)\n",
        "        regularizer = regularizer/self.batch_size\n",
        "\n",
        "        maxi = tf.log(tf.nn.sigmoid(pos_scores - neg_scores))\n",
        "\n",
        "        mf_loss = tf.negative(tf.reduce_mean(maxi))\n",
        "        reg_loss = self.decay * regularizer\n",
        "        return mf_loss, reg_loss\n",
        "\n",
        "\n",
        "    def _statistics_params(self):\n",
        "        # number of params\n",
        "        total_parameters = 0\n",
        "        for variable in self.weights.values():\n",
        "            shape = variable.get_shape()  # shape is an array of tf.Dimension\n",
        "            variable_parameters = 1\n",
        "            for dim in shape:\n",
        "                variable_parameters *= dim.value\n",
        "            total_parameters += variable_parameters\n",
        "        if self.verbose > 0:\n",
        "            print(\"#params: %d\" % total_parameters)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_id)\n",
        "\n",
        "    config = dict()\n",
        "    config['n_users'] = data_generator.n_users\n",
        "    config['n_items'] = data_generator.n_items\n",
        "\n",
        "    t0 = time()\n",
        "\n",
        "    model = BPRMF(data_config=config)\n",
        "\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    # *********************************************************\n",
        "    # save the model parameters.\n",
        "    if args.save_flag == 1:\n",
        "        weights_save_path = '%sweights/%s/%s/l%s_r%s' % (args.proj_path, args.dataset, model.model_type, str(args.lr),\n",
        "                                                         '-'.join([str(r) for r in eval(args.regs)]))\n",
        "        ensureDir(weights_save_path)\n",
        "        save_saver = tf.train.Saver(max_to_keep=1)\n",
        "\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    sess = tf.Session(config=config)\n",
        "\n",
        "    # *********************************************************\n",
        "    # reload the pretrained model parameters.\n",
        "    if args.pretrain == 1:\n",
        "        pretrain_path = '%sweights/%s/%s/l%s_r%s' % (args.proj_path, args.dataset, model.model_type, str(args.lr),\n",
        "                                                         '-'.join([str(r) for r in eval(args.regs)]))\n",
        "        ckpt = tf.train.get_checkpoint_state(os.path.dirname(pretrain_path + '/checkpoint'))\n",
        "        if ckpt and ckpt.model_checkpoint_path:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "            print('load the pretrained model parameters from: ', pretrain_path)\n",
        "\n",
        "            # *********************************************************\n",
        "            # get the performance from pretrained model.\n",
        "            users_to_test = list(data_generator.test_set.keys())\n",
        "            ret = test(sess, model, users_to_test, drop_flag=False)\n",
        "            cur_best_pre_0 = ret['recall'][0]\n",
        "\n",
        "            pretrain_ret = 'pretrained model recall=[%.5f, %.5f], precision=[%.5f, %.5f], hit=[%.5f, %.5f],' \\\n",
        "                           'ndcg=[%.5f, %.5f]' % \\\n",
        "                           (ret['recall'][0], ret['recall'][-1],\n",
        "                            ret['precision'][0], ret['precision'][-1],\n",
        "                            ret['hit_ratio'][0], ret['hit_ratio'][-1],\n",
        "                            ret['ndcg'][0], ret['ndcg'][-1])\n",
        "            print(pretrain_ret)\n",
        "        else:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            cur_best_pre_0 = 0.\n",
        "            print('without pretraining.')\n",
        "    else:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        cur_best_pre_0 = 0.\n",
        "        print('without pretraining.')\n",
        "\n",
        "\n",
        "    loss_loger, pre_loger, rec_loger, ndcg_loger, hit_loger = [], [], [], [], []\n",
        "    stopping_step = 0\n",
        "\n",
        "    for epoch in range(args.epoch):\n",
        "        t1 = time()\n",
        "        loss, mf_loss, reg_loss = 0., 0., 0.\n",
        "        n_batch = data_generator.n_train // args.batch_size + 1\n",
        "\n",
        "\n",
        "        for idx in range(n_batch):\n",
        "            # btime= time()\n",
        "            users, pos_items, neg_items = data_generator.sample()\n",
        "            _, batch_loss, batch_mf_loss, batch_reg_loss = sess.run([model.opt, model.loss, model.mf_loss, model.reg_loss],\n",
        "                               feed_dict={model.users: users, model.pos_items: pos_items,\n",
        "                                          model.neg_items: neg_items})\n",
        "            loss += batch_loss\n",
        "            mf_loss += batch_mf_loss\n",
        "            reg_loss += batch_reg_loss\n",
        "            # print(time() - btime)\n",
        "\n",
        "        if np.isnan(loss) == True:\n",
        "            print('ERROR: loss is nan.')\n",
        "            sys.exit()\n",
        "\n",
        "        # print the test evaluation metrics each 10 epochs; pos:neg = 1:10.\n",
        "        if (epoch + 1) % 10 != 0:\n",
        "            if args.verbose > 0 and epoch % args.verbose == 0:\n",
        "                perf_str = 'Epoch %d [%.1fs]: train==[%.5f=%.5f + %.5f]' % (epoch, time()-t1, loss, mf_loss, reg_loss)\n",
        "                print(perf_str)\n",
        "            continue\n",
        "\n",
        "        t2 = time()\n",
        "        users_to_test = list(data_generator.test_set.keys())\n",
        "        ret = test(sess, model, users_to_test, drop_flag=False)\n",
        "\n",
        "        t3 = time()\n",
        "\n",
        "        loss_loger.append(loss)\n",
        "        rec_loger.append(ret['recall'])\n",
        "        pre_loger.append(ret['precision'])\n",
        "        ndcg_loger.append(ret['ndcg'])\n",
        "        hit_loger.append(ret['hit_ratio'])\n",
        "\n",
        "        if args.verbose > 0:\n",
        "            perf_str = 'Epoch %d [%.1fs + %.1fs]: train==[%.5f=%.5f + %.5f], recall=[%.5f, %.5f], ' \\\n",
        "                       'precision=[%.5f, %.5f], hit=[%.5f, %.5f], ndcg=[%.5f, %.5f]' % \\\n",
        "                       (epoch, t2 - t1, t3 - t2, loss, mf_loss, reg_loss, ret['recall'][0], ret['recall'][-1],\n",
        "                        ret['precision'][0], ret['precision'][-1], ret['hit_ratio'][0], ret['hit_ratio'][-1],\n",
        "                        ret['ndcg'][0], ret['ndcg'][-1])\n",
        "            print(perf_str)\n",
        "\n",
        "        cur_best_pre_0, stopping_step, should_stop = early_stopping(ret['recall'][0], cur_best_pre_0,\n",
        "                                                                    stopping_step, expected_order='acc', flag_step=10)\n",
        "        if should_stop == True:\n",
        "            break\n",
        "        # *********************************************************\n",
        "        # save the user & item embeddings for pretraining.\n",
        "        if ret['recall'][0] == cur_best_pre_0 and args.save_flag == 1:\n",
        "            save_saver.save(sess, weights_save_path + '/weights', global_step=epoch)\n",
        "            print('save the weights in path: ', weights_save_path)\n",
        "\n",
        "\n",
        "    recs = np.array(rec_loger)\n",
        "    pres = np.array(pre_loger)\n",
        "    ndcgs = np.array(ndcg_loger)\n",
        "    hit = np.array(hit_loger)\n",
        "\n",
        "    best_rec_0 = max(pres[:, 0])\n",
        "    idx = list(pres[:, 0]).index(best_rec_0)\n",
        "\n",
        "    final_perf = \"Best Iter=[%d]@[%.1f]\\trecall=[%s], precision=[%s], hit=[%s], ndcg=[%s]\" % \\\n",
        "                 (idx, time() - t0, '\\t'.join(['%.5f' % r for r in recs[idx]]),\n",
        "                  '\\t'.join(['%.5f' % r for r in pres[idx]]),\n",
        "                  '\\t'.join(['%.5f' % r for r in hit[idx]]),\n",
        "                  '\\t'.join(['%.5f' % r for r in ndcgs[idx]]))\n",
        "    print(final_perf)\n",
        "\n",
        "    save_path = '%soutput_final/%s/%s.result' % (args.proj_path, args.dataset, model.model_type)\n",
        "    ensureDir(save_path)\n",
        "    f = open(save_path, 'a')\n",
        "\n",
        "    f.write('embed_size=%d, lr=%.4f, regs=%s, \\n\\t%s\\n' % (args.embed_size, args.lr, args.regs,\n",
        "                                                                         final_perf))\n",
        "    f.close()"
      ]
    }
  ]
}